---
title: "Data Mining with Spotify"
author: "Sebastian Deimen & Noah Giebink"
date: "25 May 2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## What kind of factors are associated with differences in music tastes across the globe?


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# packages
library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
library(DMwR2)
library(randomForest)

spot <- read.csv("spot_clean.csv")

# reading from url yields identical result. data types change if using read_csv() (tidyverse)
# test_read <- read.csv(url('https://raw.githubusercontent.com/nwgiebink/spotify-workshop/master/markdowns/spot_clean.csv'))

```

### Overview

**Step 1**: build a decision tree to classify countries using social variables. 

**Step 2**: use most important variable from Stage 1 to cluster countries (the tree in Step 3 performed better with fewer classes this way)

**Step 3**: build a decision tree to classify clustered countries by music variables (dimensions of music taste)

**Step 4**: Compare performance of decision tree in Step 3 to Random Forest

```{r}
# function: train/test split and build decision tree
decision_tree = function(labels, df, p = 0.8){

# splitting the data
split_index <- createDataPartition(labels, p=p, list = F)

train <- df[split_index,]
test <- df[-split_index, !(colnames(df) %in% c("country"))]
target <- df[-split_index, "country"]

# build the tree
tree <- rpartXse(country ~ ., train, se=0.5)
# plot the tree
plot <- prp(tree, type = 1, extra = 103, roundint = FALSE)


# prediction using the trees
pred <- predict(tree, test, type = "class")

# confusion matrix 
cm <- table(pred, target)

# error rate
error <- (1-sum(diag(cm))/sum(cm))
error <- cat("error rate (categorical features): ",error)

# have a look at the variable.importance
variable_importance = tree$variable.importance

output = list(plot, variable_importance, error)
return(output)
}

```




# Preprocessing

At first, we are going to make two sets of our spot-data: one only related to the music variables and one also including the  socio- variables. 

```{r}
# preprocessing the data set 

spot_music <- spot %>% select(track.popularity, track.explicit, danceability, key, 
                              loudness, mode, speechiness, acousticness, instrumentalness,
                              liveness, valence, tempo, country)

# trying another set of varibales due to horrible error rates for the first set
# I tried without track.popularity, makes it worse, so another try, including socio-variables

spot_music_socio <- spot %>% select(track.popularity, track.explicit, danceability, 
                                    key, loudness, mode, speechiness, acousticness,
                                    instrumentalness,
                                    liveness, valence, tempo, happiness, median_age,
                                    percent_urban, percent_internet_users, density_sqkm, 
                                    freedom, gdp, country)
```


# Step 1. Decision tree
## Split Train/Test

We split the spot_music_SOCIO data into training and test data, not using a validation set. 

```{r}
split_index <- createDataPartition(spot_music_socio$country, p= 0.8, list = F)

spot_music_socio_train <- spot_music_socio[split_index,]
spot_music_socio_features_test <- 
  spot_music_socio[-split_index, !(colnames(spot_music_socio) %in% c("country"))]
spot_music_socio_target_test <- spot_music_socio[-split_index, "country"]

```



## Why is the error rate 0?

Seems to good to be true...Let's examine the happiness variable.

```{r}
ggplot(spot_music_socio, aes(country, happiness))+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 90))

```

Each country has a single happiness value (boxplot lacks quantiles, etc) spread over each tuple for that country (by virtue of the sociopolitical data source's methods). Therefore, if at least one tuple from each country made it into both the training and test data, this could lead to a perfect error rate.

### Solution: Discretize variables and re-run decision tree

```{r}
disc <- function(x){
  cut(x, breaks = 4, 
      labels = c(1:4))}

  # apply disc fun to all dbl vars except track popularity
soc_disc <- mutate_if(spot_music_socio, is.numeric, funs(disc))
```

#### Examine distribution of levels

```{r}
soc2 <- select(soc_disc, -track.explicit, -country)
soc_long <- pivot_longer(soc2, cols = colnames(soc2),
                          names_to = 'variable', values_to = 'level')
ggplot(soc_long, aes(level))+
  geom_bar()+
  facet_wrap(~variable)+
  theme(axis.text.x = element_text(angle = 90))
```


Distribution of discretized levels.

## Socio-political tree with discretized variables

```{r}
decision_tree(soc_disc$country, soc_disc, p=0.8)
```

```{r}
# splitting the data

split_index <- createDataPartition(soc_disc$country, p= 0.8, list = F)

soc_train <- soc_disc[split_index,]
soc_test <- soc_disc[-split_index, !(colnames(soc_disc) %in% c("country"))]
soc_target <- soc_disc[-split_index, "country"]

# build the tree

ct2 <- rpartXse(country ~ ., soc_train, se=0.5)

# prediction using the trees

pred2 <- predict(ct2, soc_test, type = "class")

# have a look at the variable.importance
ct2$variable.importance

# confusion matrix 
cm2 <- table(pred2, soc_target)

# error rate
error2 <- (1-sum(diag(cm2))/sum(cm2))
cat("error rate (categorical features): ",error2)
                    
```

```{r}
prp(ct2, type = 1, extra = 103, roundint = FALSE)

```

Classification of countries using discretized social variables. We chose not to prune the tree because it already has impeccable performance on the test data. The error rate is still 0.



# Step 2. Use important variable from tree in Step 1 to cluster countries

Our goal is to classify countries by music tastes. To make results more interpretable, we clustered countries by the most important variable in the decision tree shown in Fig. 4, *median_age*, for classification (this also improved performance over a previous tree, not shown). We decided to use two k = 2 to get "old" and "young" countries. We then bound the clusters to our solely music-variable data and used this to grow the tree. 

**In essence, our question is: what are the most important music variables that distinguish 'old' countries' music taste from 'young' countries?**



```{r}
set.seed(42)
age <- spot_music_socio %>% group_by(country) %>% 
  summarise(age = mean(median_age))


ggplot(age, aes(x=age)) +
  geom_histogram(color = "#034c41",fill = "#069680",bins = 6) +
  labs(x = "Age", y="Counts") +
  theme(axis.title = element_text(size = rel(1.8))) +
  theme_bw()


# cluster countries by happiness, 3 clusters
a <- kmeans(age$age, 2)
a$cluster

age_clust <- cbind(age, a$cluster)
age_clust <- rename(age_clust, cluster = 'a$cluster')
arrange(age_clust, cluster)

young <- filter(age_clust, cluster == 1) %>%
  select(country)
old <- filter(age_clust, cluster == 2) %>%
  select(country)

age_music <- spot_music %>% mutate(cluster = 
                                      ifelse(country %in% young$country,
                                             'young', 'old'))

# age_music_grouped <- age_music %>%
#  group_by(country, cluster) %>%
#  summarise_if(is.numeric, mean) %>%
#  ungroup(age_music_grouped)
```

```{r}
# get rid of country
age_music2 <- select(age_music, -country)
# splitting the data
index_age <- sample(1:nrow(age_music2),0.8*nrow(age_music2))
train_age <- age_music2[index_age,]
test_age <- age_music2[-index_age,]

# # get rid of country
# age_music2 <- select(age_music_grouped, -country)
# # splitting the data 
# index_age <- sample(1:nrow(age_music2),0.8*nrow(age_music2))
# train_age <- age_music2[index_age,]
# test_age <- age_music2[-index_age,]

```

```{r, cache=TRUE}
# making a tree
set.seed(42)
# ct_age <- rpartXse(cluster ~ ., train_age, se=0.1)

ct_age <- rpartXse(cluster ~ ., train_age, control = list(maxdepth = 3))
# prediction using the trees

pred_age <- predict(ct_age, test_age, type = "class")

# have a look at the variable.importance
ct_age$variable.importance

# contingency tables
cm_age <- table(pred_age,test_age$cluster)

# error rate
error_age <- (1-sum(diag(cm_age))/sum(cm_age))
cat("DT on age clusters error rate: ",error_age)
```


```{r, cache=TRUE}
tree_age <- prp(ct_age, type = 1, extra = 103, roundint = FALSE)

```

Classification of young and old countries. 




## Step 4. Compare performance with Random Forest

```{r, cache=TRUE}
set.seed(42) 
# make sure all variables are factors
soc_disc$track.explicit <- as.factor(soc_disc$track.explicit)
# remove non-musical variables
mus <- soc_disc %>% select(-happiness, -median_age, -percent_urban,
                           -percent_internet_users, -density_sqkm,
                           -freedom, -gdp) %>% 
  mutate(cluster = ifelse(country %in% young$country,
                          'young', 'old')) %>%
  select(-country)
mus$cluster <- as.factor(mus$cluster)

# new train/test split for mus
mus_index <- createDataPartition(mus$cluster, p= 0.8, list = F)
mus_train <- mus[mus_index,]
mus_test <- mus[-mus_index,]

# grow the random forest
rf_tree <- randomForest(cluster ~ ., mus, ntree=500, importance=TRUE, na.action = na.omit )


# View(rf_tree$predicted)
# View(rf_tree$votes) #get the probablity of the prediction

# predict
rf_pred <- predict(rf_tree, mus_test, type = "class")
#cm_music <- table(pred_music,test_music$cluster)
# confusion matrix
rf_cm <- rf_tree$confusion

# error rate
rf_error <- (1-sum(diag(rf_cm))/sum(rf_cm))
cat("error rate of random forest: ", rf_error)

# variable importance
importance(rf_tree)

# plot variable importance
varImpPlot(rf_tree)

```

